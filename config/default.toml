# Cratos Default Configuration
# Copy to config/local.toml and customize for your environment
#
# ============================================================================
# SECURITY WARNING
# ============================================================================
# This is a DEVELOPMENT configuration with default credentials.
# DO NOT use these values in production!
#
# For production deployments:
# 1. Create config/production.toml with secure credentials
# 2. Set CRATOS_ENV=production
# 3. Use environment variables for sensitive data (API keys, passwords)
# 4. Ensure database and Redis use authentication
# 5. Bind server to 127.0.0.1 or use a reverse proxy
# ============================================================================

[server]
# SECURITY: In production, consider binding to 127.0.0.1 and using a reverse proxy
host = "127.0.0.1"
port = 8080

# Data directory for SQLite database and other local storage
# Default: ~/.cratos (or platform-specific data directory)
# data_dir = "~/.cratos"

# Note: PostgreSQL is no longer required. Cratos uses embedded SQLite.
# The database file will be created at: {data_dir}/cratos.db

[redis]
# SECURITY: Enable Redis AUTH in production
# When using docker-compose, use port 63791
url = "redis://localhost:63791"

[llm]
# Auto-detect provider from environment variables
# Priority: groq (free) > openrouter > novita > deepseek > openai > anthropic
# Or explicitly set: "openai", "anthropic", "groq", "deepseek", "openrouter", "novita", "ollama"
default_provider = "auto"

# ============================================================================
# Model Routing Configuration (Cost Optimization)
# ============================================================================
#
# Cratos uses tiered routing to minimize costs:
# - Simple tasks: FREE tier (Groq) - $0/month
# - General tasks: Low-cost (DeepSeek) - ~$0.14/1M tokens
# - Complex tasks: Premium (Anthropic) - Higher cost, better quality
#
# Cost comparison per 1M tokens:
# | Provider   | Model                    | Input    | Output   |
# |------------|--------------------------|----------|----------|
# | Groq       | llama-3.3-70b-versatile  | $0       | $0       | FREE!
# | DeepSeek   | deepseek-chat            | $0.14    | $0.28    |
# | OpenAI     | gpt-4o-mini              | $0.15    | $0.60    |
# | Anthropic  | claude-sonnet-4          | $3.00    | $15.00   |

[llm.model_routing]
# Simple tasks (classification, extraction): Use FREE tier
simple = { provider = "groq", model = "llama-3.3-70b-versatile" }
# General tasks (conversation, summarization): Use low-cost tier
general = { provider = "deepseek", model = "deepseek-chat" }
# Complex tasks (planning, code generation): Use premium tier
complex = { provider = "anthropic", model = "claude-sonnet-4-20250514" }
# Fallback when primary provider fails
fallback = { provider = "openai", model = "gpt-4o-mini" }
# Automatically downgrade on rate limits
auto_downgrade = true

# ============================================================================
# Provider Configurations
# ============================================================================

[llm.groq]
# Groq FREE tier - API key from GROQ_API_KEY env var
# FREE: 30 requests/minute, Llama 3.3 70B and other models
base_url = "https://api.groq.com/openai/v1"
default_model = "llama-3.3-70b-versatile"
timeout_ms = 60000

[llm.deepseek]
# DeepSeek ultra-low-cost - API key from DEEPSEEK_API_KEY env var
# $0.14/1M input, $0.28/1M output (95%+ cheaper than GPT-5.2)
base_url = "https://api.deepseek.com/v1"
default_model = "deepseek-chat"
timeout_ms = 120000

[llm.openai]
# API key loaded from OPENAI_API_KEY environment variable
default_model = "gpt-4o"
timeout_ms = 60000

[llm.anthropic]
# API key loaded from ANTHROPIC_API_KEY environment variable
default_model = "claude-sonnet-4-20250514"
timeout_ms = 120000

[llm.gemini]
# API key loaded from GOOGLE_API_KEY environment variable
default_model = "gemini-2.0-flash"
timeout_ms = 60000

[llm.ollama]
# Local Ollama instance
base_url = "http://localhost:11434"
default_model = "llama3.2"
timeout_ms = 300000

[llm.glm]
# ZhipuAI GLM API - API key from ZHIPU_API_KEY env var
base_url = "https://open.bigmodel.cn/api/paas/v4"
default_model = "glm-4-9b"
timeout_ms = 60000

[llm.qwen]
# Alibaba Qwen/DashScope API - API key from DASHSCOPE_API_KEY env var
base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
default_model = "qwen-turbo"
timeout_ms = 60000

[llm.openrouter]
# OpenRouter multi-provider gateway - API key from OPENROUTER_API_KEY env var
# Provides access to 100+ models including free tier
base_url = "https://openrouter.ai/api/v1"
default_model = "qwen/qwen3-32b:free"
timeout_ms = 60000

[llm.novita]
# Novita AI free tier - API key from NOVITA_API_KEY env var (free signup)
# Free models: Llama 3.2 1B/3B, Qwen2.5-7B, GLM-4-9B, GLM-Z1-9B
base_url = "https://api.novita.ai/v3/openai"
default_model = "qwen/qwen2.5-7b-instruct"
timeout_ms = 120000

[llm.routing]
# Legacy task-based model selection (deprecated, use model_routing instead)
classification = "groq:llama-3.3-70b-versatile"
planning = "anthropic:claude-sonnet-4-20250514"
code_generation = "anthropic:claude-sonnet-4-20250514"
summarization = "groq:llama-3.3-70b-versatile"

# ============================================================================
# Security Configuration
# ============================================================================

[security]
# Sandbox policy: strict | moderate | disabled
# - strict: All tool executions sandboxed
# - moderate: Only dangerous tools sandboxed (default)
# - disabled: No sandboxing (development only)
sandbox_policy = "moderate"

# Credential storage backend: auto | keychain | secret_service | encrypted_file
# - auto: Detect best backend for platform (recommended)
# - keychain: macOS Keychain
# - secret_service: Linux Secret Service (GNOME Keyring)
# - encrypted_file: AES-256 encrypted file (fallback)
credential_backend = "auto"

# Enable prompt injection detection
enable_injection_protection = true

[security.sandbox]
# Default network mode for sandboxed containers: none | bridge | host
# - none: No network access (most secure, default)
# - bridge: Isolated network
# - host: Host network (not recommended)
default_network = "none"

# Resource limits for sandboxed containers
max_memory_mb = 512
max_cpu_percent = 50
max_pids = 100
timeout_seconds = 60

[security.injection]
# Minimum threat level to block: info | low | medium | high | critical
block_threshold = "medium"

# Maximum input/output lengths (characters)
max_input_length = 100000
max_output_length = 1000000

[approval]
# Approval mode: always | risky_only | never
default_mode = "risky_only"

[replay]
# Event log retention in days
retention_days = 30
# Maximum events per execution
max_events_per_execution = 1000

[channels.telegram]
enabled = true
# Token loaded from TELEGRAM_BOT_TOKEN environment variable

[channels.slack]
enabled = false
# Tokens loaded from SLACK_BOT_TOKEN and SLACK_SIGNING_SECRET environment variables

# ============================================================================
# Discord Configuration
# ============================================================================
[channels.discord]
enabled = false
# Token loaded from DISCORD_BOT_TOKEN environment variable
# require_mention = true       # Require @bot mention in guild channels
# allowed_guilds = []          # Empty = allow all
# allowed_channels = []        # Empty = allow all

# ============================================================================
# WhatsApp Configuration (Baileys - Unofficial)
# ============================================================================
# WARNING: Baileys uses reverse-engineered WhatsApp Web protocol
# - Account BAN risk exists
# - Violates Meta Terms of Service
# - Do NOT use with important accounts
# - For business: use WhatsApp Business API instead
[channels.whatsapp]
enabled = false
bridge_url = "http://localhost:3001"
# allowed_numbers = []         # Empty = allow all

# ============================================================================
# WhatsApp Business API (Official)
# ============================================================================
[channels.whatsapp_business]
enabled = false
# Credentials loaded from environment variables:
# - WHATSAPP_ACCESS_TOKEN
# - WHATSAPP_PHONE_NUMBER_ID
# - WHATSAPP_BUSINESS_ACCOUNT_ID
# - WHATSAPP_WEBHOOK_VERIFY_TOKEN

# ============================================================================
# Voice Control Configuration
# ============================================================================
[voice]
enabled = false
language = "ko"               # "ko" | "en" | "ja" | "zh"
threshold = 0.5               # Voice detection threshold (0.0 - 1.0)
silence_duration_ms = 1500    # Silence duration to stop listening
max_duration_secs = 30        # Maximum recording duration

# Wake Word - customize the assistant name!
# Default: "크레토스" (Korean), can be anything you want
[voice.wake_word]
name = "크레토스"
alternatives = ["크라토스", "Hey Cratos"]
# sensitivity = 0.5

# TTS Voice Selection
# Korean: ko-KR-SunHiNeural, ko-KR-InJoonNeural, ko-KR-YuJinNeural
# English: en-US-JennyNeural, en-US-GuyNeural, en-GB-SoniaNeural
[voice.tts]
# voice = "ko-KR-SunHiNeural"

# STT (Speech-to-Text) Configuration
# Backend: "auto" | "api" | "local"
#   auto  = local Whisper first, API fallback (recommended)
#   api   = OpenAI Whisper API only (requires OPENAI_API_KEY, ~$0.006/min)
#   local = Local Whisper via candle (free, requires local-stt feature)
# Model: "tiny" | "base" | "small"
#   Korean requires "small" for reasonable accuracy (~85%)
[voice.stt]
backend = "auto"
model = "small"

# TTS (Edge TTS) is always free - no API key needed

# ============================================================================
# Vector Search Configuration (Semantic Search)
# ============================================================================

[vector_search]
# Enable vector search for semantic skill routing and execution history search
# Requires ~100MB model download on first run (fastembed nomic-embed-text-v1.5)
enabled = true

# Embedding dimensions (768 for nomic-embed-text-v1.5)
# Do not change unless using a different model
dimensions = 768

# ============================================================================
# Browser Automation Configuration
# ============================================================================
# Requires: npm install -g @anthropic-ai/mcp-server-playwright
# Or:       npm install -g @anthropic-ai/mcp-server-puppeteer

[browser]
enabled = true
default_engine = "playwright"  # "playwright" or "puppeteer"

[browser.playwright]
browser_type = "chromium"      # "chromium", "firefox", or "webkit"
headless = true
timeout = 30000                # Default timeout in milliseconds
viewport_width = 1280
viewport_height = 720

# ============================================================================
# Live Canvas Configuration
# ============================================================================

[canvas]
enabled = true
max_sessions = 100             # Maximum concurrent sessions

[canvas.websocket]
heartbeat_interval_secs = 30
max_message_size_kb = 1024

[canvas.ai_streaming]
enabled = true
chunk_size = 20                # Characters per streaming chunk

# ============================================================================
# Wake-on-LAN Configuration
# ============================================================================

[wol]
enabled = true
default_port = 9               # Standard WoL port
broadcast_address = "255.255.255.255"

# Named devices for convenience (optional)
# Use with: "Wake up my desktop" or "wol:desktop"
# [wol.devices]
# desktop = "AA:BB:CC:DD:EE:FF"
# server = "11:22:33:44:55:66"
# nas = "00:11:22:33:44:55"

# ============================================================================
# MCP (Model Context Protocol) Server Configuration
# ============================================================================
# MCP enables Claude to interact with external tools and data sources.
# Server definitions are in .mcp.json at the project root.
# Configure per-server settings here.

[mcp]
# Global MCP enable/disable
enabled = true
# Auto-start configured servers on startup
auto_start = true

# Notion MCP server
# Requires: NOTION_API_KEY environment variable
# Get key from: https://developers.notion.com/
[mcp.servers.notion]
enabled = false
# Optional: limit to specific databases
# allowed_databases = []

# Obsidian MCP server (via filesystem server)
# Requires: OBSIDIAN_VAULT_PATH environment variable
[mcp.servers.obsidian]
enabled = false
vault_path = "~/Documents/Obsidian/MyVault"
# File types to index for semantic search
# index_extensions = ["md", "txt"]

# Playwright browser automation
[mcp.servers.playwright]
enabled = true

# Filesystem access
[mcp.servers.filesystem]
enabled = true
# Root path for filesystem access
root = "~/cratos-workspace"

# ============================================================================
# ProactiveScheduler Configuration (24/7 Task Automation)
# ============================================================================
# Enables automated task execution based on time, events, or conditions.
# Tasks are persisted in SQLite and survive restarts.

[scheduler]
# Enable the scheduler
enabled = true
# Check interval in seconds (how often to look for due tasks)
check_interval_secs = 60
# Default retry delay for failed tasks (seconds)
retry_delay_secs = 30
# Maximum concurrent task executions
max_concurrent = 10
# Enable execution logging
logging_enabled = true

# Example scheduled tasks (uncomment to use)
# [[scheduler.tasks]]
# name = "daily_backup_reminder"
# description = "Send daily backup reminder at 9 AM"
# trigger = { type = "cron", expression = "0 9 * * *" }
# action = { type = "notification", channel = "telegram", channel_id = "YOUR_CHAT_ID", message = "Time to backup!" }
# enabled = true

# [[scheduler.tasks]]
# name = "hourly_health_check"
# description = "Check system health every hour"
# trigger = { type = "interval", seconds = 3600 }
# action = { type = "natural_language", prompt = "Check system health and report any issues" }
# enabled = true

# [[scheduler.tasks]]
# name = "cpu_alert"
# description = "Alert when CPU usage is high for 5 minutes"
# trigger = { type = "system", metric = "cpu_usage", threshold = 90.0, comparison = "greater_than", duration_secs = 300 }
# action = { type = "notification", channel = "telegram", channel_id = "YOUR_CHAT_ID", message = "CPU usage is critically high!" }
# enabled = true
