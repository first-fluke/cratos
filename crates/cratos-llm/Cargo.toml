[package]
name = "cratos-llm"
description = "LLM provider abstraction and model routing for Cratos AI assistant"
version.workspace = true
edition.workspace = true
rust-version.workspace = true
license.workspace = true
authors.workspace = true

[dependencies]
tokio.workspace = true
async-openai = { workspace = true, features = ["chat-completion"] }
reqwest.workspace = true
serde.workspace = true
serde_json.workspace = true
uuid.workspace = true
chrono.workspace = true
dirs.workspace = true
thiserror.workspace = true
anyhow.workspace = true
tracing.workspace = true
async-trait.workspace = true
lazy_static = "1.4"
tiktoken-rs = "0.9"

# Optional: Embeddings (tract = pure Rust ONNX runtime)
tract-onnx = { workspace = true, optional = true }
tokenizers = { workspace = true, optional = true }
hf-hub = { workspace = true, optional = true }

[features]
default = []
embeddings = ["tract-onnx", "tokenizers", "hf-hub"]

[dev-dependencies]
mockall.workspace = true
tokio-test.workspace = true
